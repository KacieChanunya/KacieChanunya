{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[![fraud-detection.gif](https://i.postimg.cc/zBtYZdf8/fraud-detection.gif)](https://postimg.cc/S2Mvcf8v)","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n    üìå Welcome to this notebook. The goal of this notebook is to demonstrate how to handle credit card fraud datasets for ML fraud detection. Credit card fraud datasets have several specifications that make them unique and require specialized handling.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style=\"font-size:14px; font-family:verdana;\">\n     üö® This notebook is written with separate code sections for ease of use. Each part of the code can be tested individually, facilitating experimentation and understanding.\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Dataset Information","metadata":{}},{"cell_type":"markdown","source":"#### Context:\nCredit card companies need to detect fraudulent transactions to prevent customers from being charged for unauthorized purchases.\n\n#### Content:\n- **Dataset**: Transactions made by European cardholders in September 2013.\n- **Duration**: Two days, with 492 frauds out of 284,807 transactions.\n- **Class Imbalance**: Fraudulent transactions (positive class) account for 0.172% of all transactions.\n- **Features**:\n  - Numerical input variables resulting from PCA transformation.\n  - 'Time': Seconds elapsed between each transaction and the first transaction.\n  - 'Amount': Transaction amount, suitable for cost-sensitive learning.\n\n- **Target**:\n  - 'Class': Response variable, 1 for fraud, 0 otherwise.\n  \n#### Source:\n- The dataset has been collected and analyzed by Worldline and the Machine Learning Group (MLG) of Universit√© Libre de Bruxelles (ULB) as part of a research collaboration on big data mining and fraud detection.\n- More details on the current and past projects related to fraud detection are available on the [MLG website](http://mlg.ulb.ac.be) and [ResearchGate](https://www.researchgate.net/project/Fraud-detection-5).\n\n#### Recommendations:\n- Due to class imbalance, accuracy should be measured using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful.\n  \n\n\n","metadata":{}},{"cell_type":"markdown","source":"## 2. Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:24:52.466586Z","iopub.execute_input":"2024-07-25T15:24:52.467023Z","iopub.status.idle":"2024-07-25T15:24:53.543106Z","shell.execute_reply.started":"2024-07-25T15:24:52.466982Z","shell.execute_reply":"2024-07-25T15:24:53.541792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the CSV file 'creditcard.csv' into a Pandas DataFrame named df\ndf = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:24:53.545078Z","iopub.execute_input":"2024-07-25T15:24:53.5456Z","iopub.status.idle":"2024-07-25T15:24:56.546054Z","shell.execute_reply.started":"2024-07-25T15:24:53.545562Z","shell.execute_reply":"2024-07-25T15:24:56.544456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the first few rows of the DataFrame df\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:24:56.547482Z","iopub.execute_input":"2024-07-25T15:24:56.547995Z","iopub.status.idle":"2024-07-25T15:24:56.582868Z","shell.execute_reply.started":"2024-07-25T15:24:56.547956Z","shell.execute_reply":"2024-07-25T15:24:56.581502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display a concise summary of the DataFrame df, including column names, non-null counts...\ndf.info()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:24:56.585918Z","iopub.execute_input":"2024-07-25T15:24:56.586274Z","iopub.status.idle":"2024-07-25T15:24:56.615247Z","shell.execute_reply.started":"2024-07-25T15:24:56.586246Z","shell.execute_reply":"2024-07-25T15:24:56.614143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the shape of the dataset (number of rows and columns)\nprint('Shape Of The Dataset', df.shape)\n\n# Print the unique class categories in the 'Class' column\nprint('Class Categories', df['Class'].unique())\n\n# Print the number of records with the class value 0 in the 'Class' column\nprint('Number Of Records With The Class Value 0: ', (df.Class == 0).sum())\n\n# Print the number of records with the class value 1 in the 'Class' column\nprint('Number Of Records With The Class Value 1: ', (df.Class == 1).sum())\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:24:56.616655Z","iopub.execute_input":"2024-07-25T15:24:56.617112Z","iopub.status.idle":"2024-07-25T15:24:56.628541Z","shell.execute_reply.started":"2024-07-25T15:24:56.617071Z","shell.execute_reply":"2024-07-25T15:24:56.627319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a count plot to visualize the distribution of classes in the 'Class' column of the DataFrame df\nsns.countplot(x='Class', data=df)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:24:56.630041Z","iopub.execute_input":"2024-07-25T15:24:56.630418Z","iopub.status.idle":"2024-07-25T15:24:56.893038Z","shell.execute_reply.started":"2024-07-25T15:24:56.630388Z","shell.execute_reply":"2024-07-25T15:24:56.891676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n    ‚ö†Ô∏è Credit card fraud datasets, including this one, are typically highly imbalanced because occurrences of fraud are rare compared to normal transactions. In the next sections, we will explore effective strategies for handling this imbalance.\n</div>","metadata":{}},{"cell_type":"markdown","source":"## 3. Features Selection","metadata":{}},{"cell_type":"code","source":"# Calculate the correlation between the 'Class' column and the first 30 columns \nx = df.corr()['Class'][:30]\nx","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:24:56.894618Z","iopub.execute_input":"2024-07-25T15:24:56.895036Z","iopub.status.idle":"2024-07-25T15:24:57.695885Z","shell.execute_reply.started":"2024-07-25T15:24:56.895004Z","shell.execute_reply":"2024-07-25T15:24:57.694396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the correlation coefficients between the 'Class' column and the first 30 columns \nx = df.corr()['Class'][:30]\n\n# Create a bar plot to visualize the correlation of features with the target variable 'Class'\nx.plot.bar(figsize=(16, 9), title=\"Correlation Of Features With Target Variable\", grid=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:24:57.697566Z","iopub.execute_input":"2024-07-25T15:24:57.698095Z","iopub.status.idle":"2024-07-25T15:24:59.232039Z","shell.execute_reply.started":"2024-07-25T15:24:57.698055Z","shell.execute_reply":"2024-07-25T15:24:59.230919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n    üìå Some features exhibit a negligible correlation with the target variable and will be removed in the subsequent sections. First, we'll examine the intercorrelation among variables.\n</div>","metadata":{}},{"cell_type":"code","source":"# Create a figure with a specific size for the heatmap\nplt.figure(figsize=(16, 9))\n\n# Create a heatmap to visualize the correlation matrix of the DataFrame df\nsns.heatmap(df.corr())","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:24:59.233454Z","iopub.execute_input":"2024-07-25T15:24:59.23377Z","iopub.status.idle":"2024-07-25T15:25:00.898486Z","shell.execute_reply.started":"2024-07-25T15:24:59.233744Z","shell.execute_reply":"2024-07-25T15:25:00.897022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n    üìå The only intercorrelated variable among others is the transaction Amount. However, this variable shows no correlation with the target variable, so it will also be removed.\n</div>","metadata":{}},{"cell_type":"code","source":"# Calculate the correlation coefficients between 'Class' and all columns\ny = df.corr()['Class']\n\n# Create a copy of the DataFrame df\ndf2 = df.copy()\n\n# Iterate through columns and drop those with absolute correlation less than 0.13\nfor i in df.columns:\n    if abs(y[i]) < 0.13:\n        df2.drop(columns=[i], inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:25:00.902768Z","iopub.execute_input":"2024-07-25T15:25:00.903208Z","iopub.status.idle":"2024-07-25T15:25:02.134917Z","shell.execute_reply.started":"2024-07-25T15:25:00.903174Z","shell.execute_reply":"2024-07-25T15:25:02.133338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n    üìåHere, we filter our dataset to keep only features with a correlation above 0.13.\n</div>","metadata":{}},{"cell_type":"code","source":"# Display the first few rows of the DataFrame df2\ndf2.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:25:02.136329Z","iopub.execute_input":"2024-07-25T15:25:02.136726Z","iopub.status.idle":"2024-07-25T15:25:02.157106Z","shell.execute_reply.started":"2024-07-25T15:25:02.136694Z","shell.execute_reply":"2024-07-25T15:25:02.155651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a figure with a specific size for the heatmap\nplt.figure(figsize=(16, 9))\n\n# Create a heatmap to visualize the correlation matrix of the DataFrame df2\nsns.heatmap(df2.corr(), annot=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:25:02.158753Z","iopub.execute_input":"2024-07-25T15:25:02.159224Z","iopub.status.idle":"2024-07-25T15:25:03.087704Z","shell.execute_reply.started":"2024-07-25T15:25:02.159188Z","shell.execute_reply":"2024-07-25T15:25:03.086236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the correlation coefficients between the 'Class' column \nx = df2.corr()['Class'][:9]\n\n# Create a bar plot to visualize the top correlated features with the target variable 'Class'\nx.plot.bar(figsize=(16, 9), title=\"Top Correlated Features With The Target Variable\", grid=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:25:03.089395Z","iopub.execute_input":"2024-07-25T15:25:03.089824Z","iopub.status.idle":"2024-07-25T15:25:03.557509Z","shell.execute_reply.started":"2024-07-25T15:25:03.089789Z","shell.execute_reply":"2024-07-25T15:25:03.556235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Handling Data Imbalance","metadata":{}},{"cell_type":"markdown","source":"\n\n- This dataset consists of:\n  - Number of records with the class value 0: 284,315\n  - Number of records with the class value 1: 492\n\nUsing this dataset as it is would be a fatal mistake due to its severe class imbalance. Here's why:\n\n- **Using the data as it is:**\n  - The overwhelming majority of records belong to the non-fraudulent class (class 0), making up over 99% of the dataset.\n  - Models trained on imbalanced data may prioritize accuracy on the majority class while neglecting the minority class (fraudulent transactions). This can result in poor performance in detecting fraud.\n\n- **Why oversampling is a fatal mistake:**\n  - Oversampling techniques like SMOTE (Synthetic Minority Over-sampling Technique) artificially inflate the minority class by generating synthetic examples. However, this can lead to overfitting and the introduction of noise, especially in cases where the minority class is already sparsely represented.\n\n- **Why downsampling is the best option:**\n  - Downsampling involves randomly reducing the number of samples in the majority class to balance it with the minority class. This approach helps mitigate the biases towards the majority class while maintaining the integrity of the dataset.\n  - By reducing the number of majority class samples to match the minority class, downsampling encourages the model to learn from both classes equally, improving its ability to accurately detect fraudulent transactions.\n\n","metadata":{}},{"cell_type":"markdown","source":"\n1. **Using Imbalanced Data (No Sampling) Example:**\n   - Dataset:\n     - Class 0 (non-fraudulent transactions): 284,315 records\n     - Class 1 (fraudulent transactions): 492 records\n   - Example:\n     - Accuracy on test set: 99.8%\n     - Confusion Matrix:\n       ```\n                 Predicted Non-Fraudulent    Predicted Fraudulent\n       Actual Non-Fraudulent      71,078               200\n       Actual Fraudulent              50                40\n       ```\n     - Issue: High accuracy is misleading; the model fails to detect most fraudulent transactions (low recall for class 1).\n\n2. **Oversampling (SMOTE) Example:**\n   - Dataset:\n     - Original Class 0: 284,315 records\n     - Class 1: 492 records\n     - After SMOTE (oversampling Class 1 to match Class 0):\n       - Class 0: 284,315 records\n       - Class 1: 284,315 records (synthetic)\n   - Example:\n     - Model Performance:\n       - Accuracy: 98.5%\n       - Confusion Matrix:\n         ```\n                   Predicted Non-Fraudulent    Predicted Fraudulent\n         Actual Non-Fraudulent      70,800                  478\n         Actual Fraudulent              10                   80\n         ```\n     - Issue: High accuracy but high false positives due to synthetic examples, leading to overfitting and reduced precision for fraud detection.","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style=\"font-size:14px; font-family:verdana;\">\n     ‚ö†Ô∏è Please note that we are discussing the training data, not the test data. The test data should represent \"reality.\" If the data the model will encounter in real life is imbalanced, then the model should be tested on imbalanced data. However, as explained earlier, to avoid bias in learning, the model should be trained on balanced data.\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n    üìå Having recognized why downsampling is the optimal technique for this dataset, let's proceed with downsampling our dataset.\n    We will downsample the entire dataset just to study outliers. Later, we will perform a proper split of the training and test data. The training data will be downsampled/balanced, while the test data will reflect the original data distribution to evaluate the model in a real-world scenario\n</div>","metadata":{}},{"cell_type":"code","source":"\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Separate features (X) and target (y)\nX = df2.drop('Class', axis=1)\ny = df2['Class']\n\n# Initialize RandomUnderSampler\nrus = RandomUnderSampler(random_state=42)\n\n# Fit and apply the resampler to the data\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Convert the resampled data back to a DataFrame\ndownsampled_df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Class'])], axis=1)\n\n\ndownsampled_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:25:03.559018Z","iopub.execute_input":"2024-07-25T15:25:03.5594Z","iopub.status.idle":"2024-07-25T15:25:03.857293Z","shell.execute_reply.started":"2024-07-25T15:25:03.559367Z","shell.execute_reply":"2024-07-25T15:25:03.855915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the shape of the downsampled DataFrame downsampled_df\ndownsampled_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:25:03.85898Z","iopub.execute_input":"2024-07-25T15:25:03.859589Z","iopub.status.idle":"2024-07-25T15:25:03.868927Z","shell.execute_reply.started":"2024-07-25T15:25:03.859553Z","shell.execute_reply":"2024-07-25T15:25:03.867471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Outliers?","metadata":{}},{"cell_type":"code","source":"# Create a count plot to visualize the distribution of classes in the 'Class' column of the downsampled DataFrame downsampled_df\nsns.countplot(x='Class', data=downsampled_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:25:03.870565Z","iopub.execute_input":"2024-07-25T15:25:03.871041Z","iopub.status.idle":"2024-07-25T15:25:04.111331Z","shell.execute_reply.started":"2024-07-25T15:25:03.871007Z","shell.execute_reply":"2024-07-25T15:25:04.109594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n    üìå Now that our dataset is balanced, we can move on to the next section: handling outliers. How should we approach outliers? Should we simply delete them? Let's explore.\n</div>","metadata":{}},{"cell_type":"code","source":"# Plotting using seaborn scatterplot\nsns.scatterplot(x='V11', y='V17', hue='Class', data=df2)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:25:04.113218Z","iopub.execute_input":"2024-07-25T15:25:04.113617Z","iopub.status.idle":"2024-07-25T15:25:21.289187Z","shell.execute_reply.started":"2024-07-25T15:25:04.113584Z","shell.execute_reply":"2024-07-25T15:25:21.287937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n    üìå Outliers are data points that significantly deviate from the average of a variable. In other words, they do not conform to the typical grouping observed in a specific cluster. Did you notice something interesting in this plot? The blue dots represent normal transactions, tightly clustered with very few outliers. In contrast, the orange dots represent fraudulent transactions, which do not form a distinct cluster, making outlier detection challenging. Is this pattern consistent across all variables or just in this example? Let's investigate further.\n</div>","metadata":{}},{"cell_type":"code","source":"import warnings\n\n# To ignore all warnings\nwarnings.filterwarnings('ignore')\n# Pair Plot of all variables\nsns.pairplot(downsampled_df, hue='Class')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:25:21.291141Z","iopub.execute_input":"2024-07-25T15:25:21.292133Z","iopub.status.idle":"2024-07-25T15:26:05.907519Z","shell.execute_reply.started":"2024-07-25T15:25:21.292092Z","shell.execute_reply":"2024-07-25T15:26:05.905952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n    üìå As you may have observed in this pairplot, normal transactions form a distinct cluster across all variables, whereas fraudulent transactions do not exhibit a specific cluster. This makes outlier detection extremely challenging. Attempting to manage outliers individually for each variable could result in transforming or deleting over 70% of the fraudulent transactions.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style=\"font-size:14px; font-family:verdana;\">\n     ‚ö†Ô∏è Fraudulent transactions do not exhibit clustering behavior and do not conform to a normal distribution, making outlier identification challenging.\n    \n\n<br>- This phenomenon arises because fraud does not adhere to a typical distribution pattern; in other words, fraudulent activities vary widely and do not consistently follow specific patterns.\n\n<br>- Fraudsters employ diverse methods that evolve over time. It's important to note that simply obtaining credit card details and full names is insufficient for completing transactions. Fraudsters often employ additional techniques such as SIM swapping to bypass payment verification processes.\n\n<br>- Due to these factors, datasets containing fraudulent transactions lack distinct clustering and do not adhere to a normal distribution.\n\n<br>For these reasons, no records will be deleted from fraudulent transactions. Additionally, it's important to consider that fraudulent transactions are rare, making each record valuable. \n\n<br>Note: Outliers can still be removed from normal transactions (non-fraudulent transactions).\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## 6. Proper Data Splitting","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Original imbalanced data (df2)\nX = df2.drop(columns='Class')\ny = df2['Class']\n\n# Step 1: Split the original data into training and testing sets with stratification\nX_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Step 2: Downsample only the training data to ensure no data leakage\ntrain_data = X_train_orig.copy()\ntrain_data['Class'] = y_train_orig\n\n# Separate the majority and minority classes in the training data\nmajority_class = train_data[train_data['Class'] == 0]  # Assuming 0 is the majority class\nminority_class = train_data[train_data['Class'] == 1]  # Assuming 1 is the minority class\n\n# Downsample the majority class\nmajority_downsampled = resample(majority_class, \n                                replace=False,    # sample without replacement\n                                n_samples=len(minority_class),  # match minority class size\n                                random_state=42)  # reproducible results\n\n# Combine the downsampled majority class with the minority class\ndownsampled_train_data = pd.concat([majority_downsampled, minority_class])\n\n# Step 3: Prepare the downsampled training set and the original test set\nX_train_downsampled = downsampled_train_data.drop(columns='Class')\ny_train_downsampled = downsampled_train_data['Class']","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:26:05.908869Z","iopub.execute_input":"2024-07-25T15:26:05.909239Z","iopub.status.idle":"2024-07-25T15:26:06.090454Z","shell.execute_reply.started":"2024-07-25T15:26:05.909208Z","shell.execute_reply":"2024-07-25T15:26:06.089108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style=\"font-size:14px; font-family:verdana;\">\n     ‚ö†Ô∏è To properly split the data, we need to follow these rules:\n\n- **Use a stratified split:** Since the data is highly imbalanced, a random split might result in a training or test set with no fraud transactions. Fraud transactions represent only a fraction of the dataset, so the risk of ending up with a dataset without fraud transactions is high. A stratified split ensures that this imbalance is maintained in both the training and test sets, preventing this issue.\n\n- **Downsample the training data:** As previously explained, this is important to avoid bias and ensure that the model learns effectively.\n\n- **Ensure the distribution of the test data matches the distribution in the original data:** This is crucial for evaluating the model's performance in real-world scenarios.\n\n- **Avoid data leakage:** Ensure that there is no overlap between the final training and test datasets to prevent information from the test set influencing the training process.\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"## 7. Quick Test Using LazyPredict","metadata":{}},{"cell_type":"code","source":"!pip install lazypredict\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:26:06.092203Z","iopub.execute_input":"2024-07-25T15:26:06.092583Z","iopub.status.idle":"2024-07-25T15:26:21.495611Z","shell.execute_reply.started":"2024-07-25T15:26:06.092552Z","shell.execute_reply":"2024-07-25T15:26:21.494151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lazypredict.Supervised import LazyClassifier\n\n# Step 4: Initialize LazyClassifier and fit the model\nclf = LazyClassifier(random_state=42)\n\n# Fit the models using the downsampled training data and test on the original test set\nmodels, predictions = clf.fit(X_train_downsampled, X_test_orig, y_train_downsampled, y_test_orig)\n\n# Display the performance of the models\nmodels","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:26:21.497521Z","iopub.execute_input":"2024-07-25T15:26:21.497915Z","iopub.status.idle":"2024-07-25T15:26:37.7327Z","shell.execute_reply.started":"2024-07-25T15:26:21.497882Z","shell.execute_reply":"2024-07-25T15:26:37.731318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style=\"font-size:14px; font-family:verdana;\">\n     ‚ö†Ô∏è **Avoiding the Big Trap:** Let's remember that our test data is highly imbalanced. Relying solely on accuracy as the main metric for choosing our model is a significant mistake. The proper approach is to find a model that balances accuracy, precision, recall, and F1 score. However, since detecting normal transactions is not challenging for machine learning models, we will focus on the F1 score for fraudulent transactions (Class == 1).\n\n</div>","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import NuSVC\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.metrics import f1_score\n\n# Initialize the models\nmodels = {\n    'BernoulliNB': BernoulliNB(),\n    'NuSVC': NuSVC(probability=True),  # NuSVC requires probability=True for probability estimates\n    'NearestCentroid': NearestCentroid()\n}\n\n# Fit models and calculate F1 scores\nf1_scores = {}\n\nfor name, model in models.items():\n    # Fit the model\n    model.fit(X_train_downsampled, y_train_downsampled)\n    \n    # Make predictions\n    y_pred = model.predict(X_test_orig)\n    \n    # Calculate F1 score for class == 1\n    f1_scores[name] = f1_score(y_test_orig, y_pred, pos_label=1)\n\n# Display the F1 scores for each model\nprint(\"F1 Scores for class == 1:\")\nfor name, score in f1_scores.items():\n    print(f\"{name}: {score:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:26:37.734545Z","iopub.execute_input":"2024-07-25T15:26:37.735358Z","iopub.status.idle":"2024-07-25T15:26:39.49497Z","shell.execute_reply.started":"2024-07-25T15:26:37.73531Z","shell.execute_reply":"2024-07-25T15:26:39.493904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n    üìå Here, we chose the three models with the highest accuracy. However, we compared their F1 scores for Class == 1 and obtained the following results:\n\n**F1 Scores for Class == 1:**\n- BernoulliNB: 0.0994\n- NuSVC: 0.5950\n- NearestCentroid: 0.8144\n\nOur winner is NearestCentroid, so let's proceed to the next section.\n</div>","metadata":{}},{"cell_type":"markdown","source":"## 8. Final Model","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import NearestCentroid\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Initialize the NearestCentroid model\nFRAUDFIGHTER = NearestCentroid()\n\n# Fit the model on the downsampled training data\nFRAUDFIGHTER.fit(X_train_downsampled, y_train_downsampled)\n\n# Predict on the original test data\ny_pred = FRAUDFIGHTER.predict(X_test_orig)\n\n# Generate classification report\nreport = classification_report(y_test_orig, y_pred)\nprint(\"Classification Report:\\n\", report)\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test_orig, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Non-Fraud', 'Fraud'], \n            yticklabels=['Non-Fraud', 'Fraud'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:26:39.496725Z","iopub.execute_input":"2024-07-25T15:26:39.497324Z","iopub.status.idle":"2024-07-25T15:26:40.079437Z","shell.execute_reply.started":"2024-07-25T15:26:39.497283Z","shell.execute_reply":"2024-07-25T15:26:40.07808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n    üìå FRAUDFIGHTER demonstrates excellent performance in identifying normal transactions and shows strong, though slightly less perfect, performance in detecting fraudulent transactions. The high accuracy is somewhat skewed by the imbalanced dataset, but the F1 score for fraudulent transactions indicates that the model is effective at detecting fraud while maintaining a good balance between precision and recall.\n</div>","metadata":{}},{"cell_type":"markdown","source":"## 9. Minimizing False Negatives: A Crucial Step","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style=\"font-size:14px; font-family:verdana;\">\n     ‚ö†Ô∏è In credit card fraud detection, minimizing false negatives (fraudulent transactions classified as non-fraudulent) is crucial and often as important as accuracy, if not more so. Classifying a fraudulent transaction as non-fraudulent can pose significant risks, potentially leading to financial losses and undermining trust in the detection system. The consequences of missing fraudulent transactions can be severe, as they may go undetected and lead to further fraudulent activities. Therefore, achieving a balance between high accuracy and low false negatives is essential for robust fraud detection systems.\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style=\"font-size:14px; font-family:verdana;\">\n     ‚ö†Ô∏è Example: \n    \n\n\n- Suppose a bank earns $1 profit per transaction, whether normal or fraudulent.\n- There are 100 transactions: 99 normal and 1 fraudulent.\n- Profit from 99 normal transactions: ( 99 x \\$1 = \\$99 )\n\nNow, consider the fraudulent transaction:\n- Fraudulent transaction amount: \\$100.\n- If misclassified as normal, bank earns: \\$1.\n- Actual loss to the bank: \\$100.\n\nIn this case:\n- Bank's profit from normal transactions: \\$99.\n- Loss from misclassified fraudulent transaction: \\$100.\n\nMisclassifying the fraudulent transaction as normal means:\n- Bank's total profit calculation: \\( \\$99 + \\$1 = \\$100 \\).\n- Actual loss due to fraud: \\$100.\n\nTherefore, misclassifying one fraudulent transaction as normal would result in the bank losing all the profits earned from normal transactions, resulting in a net profit of \\$0.\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n    üìå For this dataset, we didn‚Äôt find a way to reduce false negatives. However, we included the above disclaimers to make this notebook useful for all kinds of datasets. We also explained why false negatives are a significant issue for fraud detection datasets!\n</div>","metadata":{}},{"cell_type":"markdown","source":"## 10. Features Importance","metadata":{}},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\n\n\n# Compute permutation importance\nresult = permutation_importance(model, X_test_orig, y_test_orig, n_repeats=10, random_state=42, scoring='f1')\n\n# Get mean and standard deviation of importance values\nimportance = result.importances_mean\nstd = result.importances_std\n\n# Create a DataFrame for easy plotting\nimportance_df = pd.DataFrame({\n    'Feature': X_test_orig.columns,\n    'Importance': importance,\n    'Std Dev': std\n}).sort_values(by='Importance', ascending=False)\n\n# Print the DataFrame\nprint(\"Feature Importances:\\n\", importance_df)\n\n# Plot feature importances\nplt.figure(figsize=(10, 8))\nsns.barplot(x='Importance', y='Feature', data=importance_df, xerr=importance_df['Std Dev'], palette='viridis')\nplt.xlabel('Mean Importance')\nplt.ylabel('Feature')\nplt.title('Permutation Importance of Features')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T15:26:40.081504Z","iopub.execute_input":"2024-07-25T15:26:40.082023Z","iopub.status.idle":"2024-07-25T15:26:51.895091Z","shell.execute_reply.started":"2024-07-25T15:26:40.08198Z","shell.execute_reply":"2024-07-25T15:26:51.893903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-secondary\" style=\"font-size:14px; font-family:verdana; background-color:#d3d3d3; color:#555555;\">\n    üìå We assessed feature importance and found that `V3`, `V14`, and `V17` are the top contributors to our model‚Äôs performance. \n</div>","metadata":{}},{"cell_type":"markdown","source":"## 11. Conclusions ","metadata":{}},{"cell_type":"markdown","source":"\n1. **Effective Model Development:** Through meticulous data preprocessing, feature selection, and model optimization steps, we've developed a robust fraud detection model.\n   \n2. **Importance of Imbalance Handling:** Addressing the imbalance in the dataset was critical. Downsampling the majority class improved model performance by ensuring balanced representation of fraudulent and non-fraudulent transactions.\n\n3. **Model Performance:** Our final model, FRAUDFIGHTER, achieved an impressive accuracy of 99% with minimal false negatives and false positives. This underscores its capability to accurately detect fraudulent transactions.\n\n6. **Real-world Application:** The methodologies and techniques applied here are crucial for real-world applications, where the cost of misclassifying fraudulent transactions can be substantial both financially and in terms of trust and customer satisfaction.\n\n7. **Future Directions:** Further enhancements could involve exploring advanced feature engineering techniques, integrating additional data sources, or deploying the model in a real-time environment to continuously improve fraud detection capabilities.","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n    üìå üëã Thank you for reading this notebook! If you found this content useful, please consider giving it an upvote. Your support is greatly appreciated! üåü.\n</div>","metadata":{}}]}